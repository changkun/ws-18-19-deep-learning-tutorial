{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow\n",
    "## What is  TensorFlow?\n",
    "<it>TensorFlow</it> is a powerful open source software library for numerial computation, particularly optimized and fine-tuned for large-scale Machine Learning problems.\n",
    "\n",
    "The basic principle of TensorFlow is as follows: first, we define in python a graph of computations which will be executed. TensorFlow then takes that graph and runs it efficiently using optimized C++ code.\n",
    "\n",
    "To deal also with large-scale problems, the graph is broken up into several chunks such that they can be computed in parallel across multiple CPUs or GPUs. This makes it possible for TensorFlow to run and train a network with millions of parameters on a training set composed of billions of instances with millions of features each. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Assuming you installed Jupyter and Scikit-learn, you can simply use <em>pip install</em> to install TensorFlow. If you created an isolated environment using <it>virtualenv</it>, you first need to activate the environment for which you would like to install TensforFlow.\n",
    "\n",
    "- cd \\$your_env // path to your working directory\n",
    "- source env/bin/activate\n",
    "\n",
    "Next, install TensorFlow:\n",
    "- pip3 install --upgrade tensorflow\n",
    "\n",
    "Side note: if you would like to have GPU support, youe need to install tensorflow-gpu instead of tensorflow. For our basic introduction no GPU support is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Arithmetic\n",
    "First we execute some elementary TensorFlow computational graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schmoll/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage of tf.Variables\n",
    "In the  following cell, we define two Variables and a function. Most important to understand is that the functions <em>fnc</em> is not caluclated by the following three lines. It just created a computation graph. In fact, even the variables are not initialized yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.Variable(3, name=\"x1\")\n",
    "x2 = tf.Variable(6, name=\"x2\")\n",
    "fnc = x1*2*x2 + x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a session in TensorFlow?\n",
    "\n",
    "To actually execute the computation, what we need to do is to open a TensforFlow $session$. Within that, we can initialize the variables and evaluate <em>fnc</em>. A TensorFlow $session$ handles the distribution of operations onto computational units such as CPUs and GPUs and runs them. In addition to that, it keeps the variables values stored. In the following cell, we create a session, initialize the variables and evaluate the function <em>fnc</em>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "session.run(x1.initializer)\n",
    "session.run(x2.initializer)\n",
    "result = session.run(fnc)\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, the session can be closed which frees up any resources which have been used in that session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more handy way of generating a session without having to repeat $sess.run()$ all the time is by using the following structure. Notice that at the end of the block the session is also automatically closed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    x1.initializer.run()\n",
    "    x2.initializer.run()\n",
    "    result=fnc.eval()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One further optimization for this kind of code is to use a global initializer for initializing all variables. Therefore we can use the global_varaibles_initializer() function. Again, this does not perform the initialization imediately, but rather creates a node in the computation graph that indicates that all variables will be initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    init.run()\n",
    "    result = fnc.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage of tf.placeholders\n",
    "\n",
    "In case that our values are changing during our computation, we need to specify placeholder nodes instead of variables. These nodes are different as they don't actually perform any computation, they just output the data you tell them to output at runtime. They are typically used to pass the training data to TensorFlow during training (e.g., mini-batches). If at runtime the values for a placeholder is not specified TensorFlow throws an exception. The next cell shows how we can easily create placeholders having a specific type being attached in the parameter list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = tf.placeholder(tf.float32)\n",
    "y2 = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say, we want to add/multiply those two values being stored in the placeholders $y1$ and $y2$. Therefore, we take usage of two tensorflow operations $tf.add(\\cdot,\\cdot)$ and $tf.multiply(\\cdot,\\cdot)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_op = tf.add(y1, y2)\n",
    "product_op = tf.multiply(y1, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can again evaluate the two operations within a session. We use the feed_dict of the $session.run(\\cdot)$ to feed the data to our code. We specify the values by a key being the reference to our placheolder node and as values the actual value of the placeholder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    sum_result = session.run(sum_op, feed_dict={y1: 36.0, y2: 6.0})\n",
    "    product_result = session.run(product_op, feed_dict={y1: 6.0, y2: 21.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.0\n",
      "126.0\n"
     ]
    }
   ],
   "source": [
    "print (sum_result)\n",
    "print (product_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basic arrray arithemtic using tf.placeholders\n",
    "\n",
    "When we create a placeholer node we can optionally also specify its shape, if there is need to do that. If the dimension of the placeholder is not given (None), then it means that the placeholder is of \"any size\". The following cell show that we can also feed arrays to our two placeholders $y1$ and $y2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    sum_result = session.run(sum_op, feed_dict={y1: [6.0, 4.0, 2.0], y2: [3, 2, 1.0]})\n",
    "    product_result = session.run(product_op, feed_dict={y1: [2.0, 4.0], y2: 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9. 6. 3.]\n",
      "[1. 2.]\n"
     ]
    }
   ],
   "source": [
    "print (sum_result)\n",
    "print (product_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4-3\n",
    "\n",
    "In this exercise we will classify handwritten digits with convolutional neural networks using tensorflow.\n",
    "Therefor we use the public dataset MNSIT.\n",
    "In tensorflow one can simply download this dataset with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True) # replace \"/tmp/data/\" with a folder on your system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other useful imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step we want to create placeholders for the input and the output variables. However, before we do so, let's check the input shape of the dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the x and y placeholders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder('float', [None, 784])\n",
    "y = tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we want to write a function that returns a CNN model with the following layers (you may use methods of the tf.nn module: https://www.tensorflow.org/api_docs/python/tf/nn):\n",
    "* Convolutional Layer: kernel size 5x5, filters=8, stride=1, padding='SAME', activation=RELU\n",
    "* Convolutional Layer: kernel size 3x3, filters=8, stride=1, padding='SAME', activation=RELU\n",
    "* Fully Connected Layer: activation=RELU, output neurons=256\n",
    "* Output Layer: activation=Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(x, classes=10):\n",
    "    # first reshape the input to a 2d image\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    w = tf.Variable(tf.random_normal([5,5,1,8])) # [filter_height, filter_width, in_channels, out_channels]\n",
    "    conv1 = tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    \n",
    "    w = tf.Variable(tf.random_normal([3,3,8,8])) # [filter_height, filter_width, in_channels, out_channels]\n",
    "    conv2 = tf.nn.conv2d(conv1, w, strides=[1,1,1,1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    \n",
    "    # we need to flatten / reshape the output of the cnn\n",
    "    w = tf.Variable(tf.random_normal([28*28*8,256]))\n",
    "    bias = tf.Variable(tf.random_normal([256]))\n",
    "    fc = tf.reshape(conv2, [-1, 8*28*28])\n",
    "    fc = tf.matmul(fc, w)\n",
    "    fc = fc + bias\n",
    "    fc = tf.nn.relu(fc)\n",
    "    \n",
    "    w = tf.Variable(tf.random_normal([256, classes]))\n",
    "    bias = tf.Variable(tf.random_normal([classes]))\n",
    "    output = tf.matmul(fc, w) + bias\n",
    "    # softmax activation will be done by softmax_cross_entropy_with_logits_v2\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that optimizes the weights using the Adam optimizer (tf.train.AdamOptimizer) with respect to the cross entropy loss function (tf.nn.softmax_cross_entropy_with_logits_v2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, model, epochs=10, batch_size=128):\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initializers.global_variables())\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for batch in range(mnist.train.num_examples//batch_size):\n",
    "                x_train, y_train = mnist.train.next_batch(batch_size)\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x:x_train, y:y_train})\n",
    "                epoch_loss += c\n",
    "            \n",
    "            print(\"Epoch %d / %d completed. Loss: %.3f\" % (epoch+1, epochs, epoch_loss))\n",
    "        \n",
    "        # compute accuracy:\n",
    "        correct = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('Accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10 completed. Loss: 156387.619\n",
      "Epoch 2 / 10 completed. Loss: 31303.223\n",
      "Epoch 3 / 10 completed. Loss: 17285.855\n",
      "Epoch 4 / 10 completed. Loss: 10732.013\n",
      "Epoch 5 / 10 completed. Loss: 7051.961\n",
      "Epoch 6 / 10 completed. Loss: 4323.925\n",
      "Epoch 7 / 10 completed. Loss: 2757.528\n",
      "Epoch 8 / 10 completed. Loss: 1599.441\n",
      "Epoch 9 / 10 completed. Loss: 939.234\n",
      "Epoch 10 / 10 completed. Loss: 573.898\n",
      "Accuracy: 0.9478\n",
      "CPU times: user 16min 40s, sys: 2min 32s, total: 19min 13s\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "model = cnn_model(x)\n",
    "%time train(x, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to add a max-pooling layer after each convolutional layer.\n",
    "Complete the function below such that it creates the following model:\n",
    "* Convolutional Layer: kernel size 5x5, filters=8, stride=1, padding='SAME', activation=RELU\n",
    "* Max-Pooling layer: kernel size: 2, stride=2, padding='SAME'\n",
    "* Convolutional Layer: kernel size 3x3, filters=8, stride=1, padding='SAME', activation=RELU\n",
    "* Max-Pooling layer: kernel size: 2, stride=2, padding='SAME'\n",
    "* Fully Connected Layer: activation=RELU, output neurons=256\n",
    "* Output Layer: activation=Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_pooling(x, classes=10):\n",
    "    # first reshape the input to a 2d image\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    w = tf.Variable(tf.random_normal([5,5,1,8])) # [filter_height, filter_width, in_channels, out_channels]\n",
    "    conv1 = tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    \n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    \n",
    "    w = tf.Variable(tf.random_normal([3,3,8,8])) # [filter_height, filter_width, in_channels, out_channels]\n",
    "    conv2 = tf.nn.conv2d(pool1, w, strides=[1,1,1,1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    \n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    \n",
    "    # we need to flatten / reshape the output of the cnn\n",
    "    # due to the max-pooling operations the image size reduced to 28/2/2 = 7\n",
    "    w = tf.Variable(tf.random_normal([7*7*8,256]))\n",
    "    bias = tf.Variable(tf.random_normal([256]))\n",
    "    fc = tf.reshape(pool2, [-1, 8*7*7])\n",
    "    fc = tf.matmul(fc, w)\n",
    "    fc = fc + bias\n",
    "    fc = tf.nn.relu(fc)\n",
    "    \n",
    "    w = tf.Variable(tf.random_normal([256, classes]))\n",
    "    bias = tf.Variable(tf.random_normal([classes]))\n",
    "    output = tf.matmul(fc, w) + bias\n",
    "    # softmax activation will be done by softmax_cross_entropy_with_logits_v2\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10 completed. Loss: 178734.735\n",
      "Epoch 2 / 10 completed. Loss: 28778.201\n",
      "Epoch 3 / 10 completed. Loss: 16936.463\n",
      "Epoch 4 / 10 completed. Loss: 11975.147\n",
      "Epoch 5 / 10 completed. Loss: 8957.668\n",
      "Epoch 6 / 10 completed. Loss: 7046.698\n",
      "Epoch 7 / 10 completed. Loss: 5783.312\n",
      "Epoch 8 / 10 completed. Loss: 4742.917\n",
      "Epoch 9 / 10 completed. Loss: 3976.335\n",
      "Epoch 10 / 10 completed. Loss: 3299.564\n",
      "Accuracy: 0.9547\n",
      "CPU times: user 8min 20s, sys: 1min 7s, total: 9min 28s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "model = cnn_model_pooling(x)\n",
    "%time train(x, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you observe?\n",
    "\n",
    "* much less parameters\n",
    "* faster training\n",
    "* better results with the same amount of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inception\n",
    "\n",
    "Inception is a well performing technique that combines the outputs of three different kind of filters (1x1, 3x3, 5x5) and a 3x3 max pooling.\n",
    "\n",
    "Implement a function that appends an inception module to a given input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception(x, filers_per_conv=8):\n",
    "    in_channels = int(x.shape[3])\n",
    "    \n",
    "    w = tf.Variable(tf.random_normal([1,1,in_channels,filers_per_conv]))\n",
    "    conv1x1 = tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME')\n",
    "    \n",
    "    w = tf.Variable(tf.random_normal([3,3,in_channels,filers_per_conv]))\n",
    "    conv3x3 = tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME')\n",
    "    \n",
    "    w = tf.Variable(tf.random_normal([5,5,in_channels,filers_per_conv]))\n",
    "    conv5x5 = tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME')\n",
    "    \n",
    "    pool = tf.nn.max_pool(x, ksize=[1,3,3,1], strides=[1,1,1,1], padding='SAME')\n",
    "    \n",
    "    out = tf.concat([conv1x1, conv3x3, conv5x5, pool], 3)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_reduction1x1conv(x, out_dim):\n",
    "    # [filter_height, filter_width, in_channels, out_channels]\n",
    "    w = tf.Variable(tf.random_normal([1,1,int(x.shape[3]),out_dim])) \n",
    "    conv = tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME')\n",
    "    conv = tf.nn.relu(conv)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_inception(x, classes=10):\n",
    "    # first reshape the input to a 2d image\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    conv = tf.nn.relu(inception(x, 8))\n",
    "    conv = dim_reduction1x1conv(conv, 8)\n",
    "    conv = tf.nn.relu(inception(conv, 16))\n",
    "    conv = dim_reduction1x1conv(conv, 16)\n",
    "    \n",
    "    # we need to flatten / reshape the output of the cnn\n",
    "    w = tf.Variable(tf.random_normal([int(np.prod(conv.shape[1:])),512]))\n",
    "    bias = tf.Variable(tf.random_normal([512]))\n",
    "    fc = tf.reshape(conv, [-1, np.prod(conv.shape[1:])])\n",
    "    fc = tf.matmul(fc, w)\n",
    "    fc = fc + bias\n",
    "    fc = tf.nn.relu(fc)\n",
    "    \n",
    "    w = tf.Variable(tf.random_normal([512, classes]))\n",
    "    bias = tf.Variable(tf.random_normal([classes]))\n",
    "    output = tf.matmul(fc, w) + bias\n",
    "    # softmax activation will be done by softmax_cross_entropy_with_logits_v2\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10 completed. Loss: 9059833.742\n",
      "Epoch 2 / 10 completed. Loss: 1493355.659\n",
      "Epoch 3 / 10 completed. Loss: 739177.497\n",
      "Epoch 4 / 10 completed. Loss: 410815.617\n",
      "Epoch 5 / 10 completed. Loss: 257497.780\n",
      "Epoch 6 / 10 completed. Loss: 148161.487\n",
      "Epoch 7 / 10 completed. Loss: 91139.134\n",
      "Epoch 8 / 10 completed. Loss: 61463.640\n",
      "Epoch 9 / 10 completed. Loss: 48070.501\n",
      "Epoch 10 / 10 completed. Loss: 45572.962\n",
      "Accuracy: 0.9643\n",
      "CPU times: user 1h 18min 3s, sys: 13min 6s, total: 1h 31min 9s\n",
      "Wall time: 16min 33s\n"
     ]
    }
   ],
   "source": [
    "model = cnn_model_inception(x)\n",
    "%time train(x, y, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
